---
title: 'Blog Post 10: Post-Election Reflection'
author: John Kulow
date: '2024-11-18'
slug: blog-post-10-post-election-reflection
categories: []
tags: []
---

```{r setup, warning = FALSE, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, messages = FALSE, warning = FALSE)
```

```{r libraries, include=FALSE}
library(blogdown)
library(car)
library(caret)
library(curl)
library(CVXR)
library(foreign)
library(geofacet)
library(ggpubr)
library(ggthemes)
library(glmnet)
library(gridExtra)
library(haven)
library(janitor)
library(kableExtra)
library(knitr)
library(maps)
library(mgcv)
library(mgcViz)
library(mlr3)
library(patchwork)
library(randomForest)
library(ranger)
library(RColorBrewer)
library(readxl)
library(rstan)
library(scales)
library(sf)
library(shinystan)
library(sjPlot)
library(spData)
library(stargazer)
library(tidygeocoder)
library(tidyverse)
library(tigris)
library(tmap)
library(tmaptools)
library(viridis)
library(censable)
library(readstata13)
```

```{r importing data, include=FALSE}
####----------------------------------------------------------#
#### Read, merge, and process data.
####----------------------------------------------------------#

# Read popular vote datasets. 
d_popvote <- read_csv("data/popvote_1948_2020.csv")
d_state_popvote <- read_csv("data/state_popvote_1948_2020.csv")
d_state_popvote[d_state_popvote$state == "District of Columbia",]$state <- "District Of Columbia"

# Read elector distribution dataset. 
d_ec <- read_csv("data/corrected_ec_1948_2024.csv")

# Read polling data. 
d_polls <- read_csv("data/national_polls_1968-2024.csv")
d_state_polls <- read_csv("data/state_polls_1968-2024.csv")

# Read turnout data. 
d_turnout <- read_csv("data/state_turnout_1980_2022.csv")

# Read county turnout. 
d_county_turnout <- read_csv("data/county_turnout.csv")

# Read state-level demographics.
d_state_demog <- read_csv("data/demographics.csv")

# Read county demographics. 
d_county_demog <- read_csv("data/county_demographics.csv")

# Read campaign events datasets. 
d_campaign_events <- read_csv("data/campaigns_2016_2024.csv")[,-1]

d_pollav_nat <- read_csv("data/national_polls_1968-2024.csv")
d_pollav_state <- read_csv("data/state_polls_1968-2024.csv")

# Read 2024 results datasets. 
d_state_2024 <- read_csv("data/state_votes_pres_2024.csv")[-1, 1:6]
d_county_2024 <- read_csv("data/county_votes_pres_2024.csv")[-1, 1:6]
d_county_2020 <- read_csv("data/county_votes_pres_2020.csv")[-1, 1:6]

# Process 2024 state and county-level data. 
d_state_2024 <- d_state_2024 |> 
  mutate(FIPS = as.numeric(FIPS), 
         votes_trump = as.numeric(`Donald J. Trump`), 
         votes_harris = as.numeric(`Kamala D. Harris`), 
         votes = as.numeric(`Total Vote`), 
         trump_pv = votes_trump/votes, 
         harris_pv = votes_harris/votes, 
         trump_2pv = votes_trump/(votes_trump + votes_harris), 
         harris_2pv = votes_harris/(votes_trump + votes_harris)) |> 
  mutate(winner = case_when(votes_trump > votes_harris ~ "REP", 
                            .default = "DEM"),
         state = `Geographic Name`) |> 
  select(FIPS, state, `Geographic Name`, `Geographic Subtype`, votes_trump, votes_harris, votes, 
         winner, trump_pv, harris_pv, trump_2pv, harris_2pv)

d_county_2024 <- d_county_2024 |>
  mutate(FIPS = as.numeric(FIPS),
         votes_trump = as.numeric(`Donald J. Trump`), 
         votes_harris = as.numeric(`Kamala D. Harris`), 
         votes = as.numeric(`Total Vote`), 
         trump_pv = votes_trump/votes, 
         harris_pv = votes_harris/votes, 
         trump_2pv = votes_trump/(votes_trump + votes_harris), 
         harris_2pv = votes_harris/(votes_trump + votes_harris)) |> 
  mutate(winner = case_when(votes_trump > votes_harris ~ "REP", 
                            .default = "DEM")) |> 
  select(FIPS, `Geographic Name`, `Geographic Subtype`, votes_trump, votes_harris, votes, 
         winner, trump_pv, harris_pv, trump_2pv, harris_2pv)

d_county_2020 <- d_county_2020 |> 
  mutate(FIPS = as.numeric(FIPS),
         votes_trump_2020 = as.numeric(`Donald J. Trump`), 
         votes_biden_2020 = as.numeric(`Joseph R. Biden Jr.`), 
         votes_2020 = as.numeric(`Total Vote`), 
         trump_pv_2020 = votes_trump_2020/votes_2020, 
         biden_pv_2020 = votes_biden_2020/votes_2020, 
         trump_2pv_2020 = votes_trump_2020/(votes_trump_2020 + votes_biden_2020), 
         biden_2pv_2020 = votes_biden_2020/(votes_trump_2020 + votes_biden_2020)) |> 
  mutate(winner_2020 = case_when(votes_trump_2020 > votes_biden_2020 ~ "REP", 
                            .default = "DEM")) |> 
  select(FIPS, `Geographic Name`, `Geographic Subtype`, votes_trump_2020, votes_biden_2020, votes_2020, 
         winner_2020, trump_pv_2020, biden_pv_2020, trump_2pv_2020, biden_2pv_2020)
```

```{r nat model, include=FALSE}
d_poll_nat <- d_pollav_nat |>
  select(year, state, weeks_left, poll_date, poll_support, party) |>
  filter(month(poll_date) %in% c(9, 10)) |>
  group_by(year, party, month = month(poll_date)) |>
  summarize(poll_support = round(weighted.mean(poll_support, weeks_left, na.rm = TRUE), 2), .groups = "drop") |>
  pivot_wider(names_from = month, 
              values_from = poll_support, 
              names_prefix = "month_") |>
  drop_na() |>
  rename(sept_poll = month_9, 
         oct_poll = month_10)

d_fred <- read_csv("data/fred_econ.csv")

d_fred_2 <- d_fred |>
  filter(quarter == 2)

d_nat_model <- d_popvote |> 
  mutate(party = if_else(party == "democrat", "DEM", "REP")) |>
  left_join(d_fred_2, by = "year") |> 
  filter(year > 1967, 
         year != 2020,
         incumbent_party) |>
  mutate(incumbent = as.numeric(incumbent)) |>
  left_join(d_poll_nat, by = c("year", "party"))

d_nat_reg <- lm(pv2p ~ oct_poll + sept_poll + GDP_growth_quarterly + incumbent, 
                data = d_nat_model)
```

```{r nat model validation, include=FALSE}
set.seed(02138)

out_samp_errors <- sapply(1:1000, function(i) {
  years_out_samp <- sample(d_nat_model$year, 7)
  mod <- lm(pv2p ~ sept_poll + oct_poll + GDP_growth_quarterly + incumbent, 
            data = d_nat_model[!(d_nat_model$year %in% years_out_samp),])
  out_samp_pred <- predict(mod, d_nat_model[d_nat_model$year %in% years_out_samp,])
  if (any(is.na(out_samp_pred)) || any(is.na(d_nat_model$pv2p[d_nat_model$year %in% years_out_samp]))) {
    return(NA)
  }
  out_samp_truth <- d_nat_model$pv2p[d_nat_model$year %in% years_out_samp]
  mean(abs(out_samp_pred - out_samp_truth))
})
out_samp_errors <- na.omit(out_samp_errors)
mean_out_samp_error <- mean(abs(out_samp_errors))

nat_model_valid_plot <- ggplot() +
  geom_histogram(aes(x = out_samp_errors), fill = "lightgray", binwidth = 0.5) + 
  geom_vline(aes(xintercept = mean_out_samp_error), color = "black") +
  labs(title = "Out-of-Sample Cross Validation Error Distribution",
       x = "Mean Absolute Errors",
       y = "") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 14)
  ) +
  scale_x_continuous(limits = c(0, 20))
```

```{r nat pred, include=FALSE}
d_nat_model_2024 <- d_nat_model |>
  filter(year == 2024) |>
  select(sept_poll, oct_poll, GDP_growth_quarterly, juneapp) |>
  mutate(incumbent = 0)

nat_pred_2024 <- predict(d_nat_reg, d_nat_model_2024, interval = "prediction", level = 0.95)
```

```{r state model, include=FALSE}
nat_poll_d_yearly <- d_pollav_nat |>
  select(year, state, weeks_left, poll_date, poll_support, party) |>
  filter(month(poll_date) %in% c(9, 10)) |>
  group_by(year, party, month = month(poll_date)) |>
  summarize(poll_support = round(weighted.mean(poll_support, weeks_left, na.rm = TRUE), 2), .groups = "drop") |>
  pivot_wider(names_from = month, 
              values_from = poll_support, 
              names_prefix = "month_") |>
  drop_na() |>
  rename(nat_sept_poll = month_9, 
         nat_oct_poll = month_10) |>
  filter(party == "DEM")

d_poll_state <- d_pollav_state |>
  select(year, state, weeks_left, poll_date, poll_support, party) |>
  filter(month(poll_date) %in% c(9, 10),
         party == "DEM", 
         year > 1999) |>
  group_by(year, state, month = month(poll_date)) |>
  summarize(poll_support = round(weighted.mean(poll_support, weeks_left, na.rm = TRUE), 2), .groups = "drop") |>
  pivot_wider(names_from = month, 
              values_from = poll_support, 
              names_prefix = "month_") |>
  left_join(d_state_popvote, by = c("year", "state")) |>
  select(year, state, month_9, month_10, D_pv2p, D_pv2p_lag1, D_pv2p_lag2) |>
  drop_na() |>
  rename(sept_poll = month_9, 
         oct_poll = month_10) |>
  left_join(d_fred_2, by = "year") |>
  mutate(d_incumbent = case_when(year == "2000" ~ 1,
                                 year == "2004" ~ 0,
                                 year == "2008" ~ 0,
                                 year == "2012" ~ 1,
                                 year == "2016" ~ 1,
                                 year == "2020" ~ 0,
                                 year == "2024" ~ 1),
         D_pv2p_lag_shift = D_pv2p_lag1 - D_pv2p_lag2) |>
  left_join(nat_poll_d_yearly, by = "year")

d_state_reg <- lm(D_pv2p ~ oct_poll + sept_poll + D_pv2p_lag1 + D_pv2p_lag2 + d_incumbent + nat_oct_poll,
                data = d_poll_state)
```

```{r state model 2, include=FALSE}
d_nopoll_states <- d_state_popvote |>
  select(year, state, D_pv2p, D_pv2p_lag1, D_pv2p_lag2) |>
  left_join(d_fred_2, by = "year") |>
  mutate(d_incumbent = case_when(year == "1948" ~ 1,
                                 year == "1952" ~ 1,
                                 year == "1956" ~ 0,
                                 year == "1960" ~ 0,
                                 year == "1964" ~ 1,
                                 year == "1968" ~ 1,
                                 year == "1972" ~ 0,
                                 year == "1976" ~ 0,
                                 year == "1980" ~ 1,
                                 year == "1984" ~ 0,
                                 year == "1988" ~ 0,
                                 year == "1992" ~ 0,
                                 year == "1996" ~ 1,
                                 year == "2000" ~ 1,
                                 year == "2004" ~ 0,
                                 year == "2008" ~ 0,
                                 year == "2012" ~ 1,
                                 year == "2016" ~ 1,
                                 year == "2020" ~ 0,
                                 year == "2024" ~ 1)) |>
  filter(year > 2000) |>
  left_join(nat_poll_d_yearly, by = "year")

d_nopoll_state_reg <- lm(D_pv2p ~ D_pv2p_lag1 + D_pv2p_lag2 + d_incumbent + nat_oct_poll + nat_sept_poll,
                data = d_nopoll_states)
```

```{r state model validation, include=FALSE}
set.seed(02138)

out_samp_errors_st <- sapply(1:1000, function(i) {
  years_out_samp_st <- sample(d_poll_state$year, 3)
  mod_st <- lm(D_pv2p ~ sept_poll + oct_poll + D_pv2p_lag1 + D_pv2p_lag2 + d_incumbent + nat_oct_poll, 
            data = d_poll_state[!(d_poll_state$year %in% years_out_samp_st),])
  out_samp_pred_st <- predict(mod_st, d_poll_state[d_poll_state$year %in% years_out_samp_st,])
  if (any(is.na(out_samp_pred_st)) || any(is.na(d_poll_state$D_pv2p[d_poll_state$year %in% years_out_samp_st]))) {
    return(NA)
  }
  out_samp_truth_st <- d_poll_state$D_pv2p[d_poll_state$year %in% years_out_samp_st]
  mean(abs(out_samp_pred_st - out_samp_truth_st))
})
out_samp_errors_st <- na.omit(out_samp_errors_st)
mean_out_samp_error_st <- mean(abs(out_samp_errors_st))

state_model_valid_plot <- ggplot() +
  geom_histogram(aes(x = out_samp_errors_st), fill = "lightgray", binwidth = 0.25) + 
  geom_vline(aes(xintercept = mean_out_samp_error_st), color = "black") +
  labs(title = "Out-of-Sample Cross Validation Errors",
       x = "Mean Absolute Errors",
       y = "") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 14)
  ) +
  scale_x_continuous(limits = c(0, 8))
```

```{r state pred, include=FALSE}
d_state_model_2024 <- d_pollav_state |>
  select(year, state, weeks_left, poll_date, poll_support, party) |>
  filter(month(poll_date) %in% c(9, 10),
         party == "DEM") |>
  group_by(year, state, month = month(poll_date)) |>
  summarize(poll_support = round(weighted.mean(poll_support, weeks_left, na.rm = TRUE), 2), .groups = "drop") |>
  pivot_wider(names_from = month, 
              values_from = poll_support, 
              names_prefix = "month_") |>
  left_join(d_state_popvote, by = c("year", "state")) |>
  arrange(state, year) |>
  group_by(state) |>
  mutate(D_pv2p_lag1 = lag(D_pv2p, n = 1),
         D_pv2p_lag2 = lag(D_pv2p, n = 2)) |>
  ungroup() |>
  select(year, state, month_9, month_10, D_pv2p, D_pv2p_lag1, D_pv2p_lag2) |>
  rename(sept_poll = month_9, 
         oct_poll = month_10) |>
  left_join(d_fred_2, by = "year") |>
  mutate(d_incumbent = case_when(year == "2008" ~ 0,
                                 year == "2012" ~ 1,
                                 year == "2016" ~ 1,
                                 year == "2020" ~ 0,
                                 year == "2024" ~ 1)) |>
  filter(year == "2024") |>
  mutate(sept_poll = case_when(is.na(sept_poll) == 1 ~ oct_poll,
                               TRUE ~ sept_poll),
         D_pv2p_lag1 = case_when(state == "Nebraska Cd 2" ~ 53.27,
                                 state == "Maine Cd 2" ~ 46.86,
                                 TRUE ~ D_pv2p_lag1),
         D_pv2p_lag2 = case_when(state == "Nebraska Cd 2" ~ 48.99,
                                 state == "Maine Cd 2" ~ 45.12,
                                 TRUE ~ D_pv2p_lag2)) |>
  left_join(nat_poll_d_yearly, by = "year")

state_pred_2024 <- predict(d_state_reg, d_state_model_2024, interval = "prediction")

d_state_model_2024 <- d_state_model_2024 |>
  mutate(predicted_values = state_pred_2024[, "fit"],
         lower_bounds = state_pred_2024[, "lwr"],
         upper_bounds = state_pred_2024[, "upr"])
```

```{r combined state pred and graphic setup, include=FALSE}
states <- unique(d_state_popvote$state)  # List of all 50 states
year_2024 <- 2024
new_rows <- expand.grid(year = year_2024, state = states)

# Join with historical data to get D_pv2p_lag1 and D_pv2p_lag2
new_rows <- new_rows |>
  left_join(d_state_popvote |>
              filter(year %in% c(2020, 2016)) |>
              select(state, year, D_pv2p) |>
              arrange(state, year) |>
              group_by(state) |>
              summarize(
                D_pv2p_lag1 = D_pv2p[year == 2020],  # Lag 1 from 2020
                D_pv2p_lag2 = D_pv2p[year == 2016]   # Lag 2 from 2016
              ),
            by = "state")

# Add new rows to the original dataset
d_state_popvote_2024 <- bind_rows(d_state_popvote, new_rows)

d_nopoll_state_model_2024 <- d_state_popvote_2024 |>
  select(year, state, D_pv2p, D_pv2p_lag1, D_pv2p_lag2) |>
  left_join(d_fred_2, by = "year") |>
  mutate(d_incumbent = case_when(year == "2016" ~ 1,
                                 year == "2020" ~ 0,
                                 year == "2024" ~ 1),
         d_incumbent_cand = case_when(year == "2016" ~ 0,
                                 year == "2020" ~ -1,
                                 year == "2024" ~ 0)) |>
  filter(year == 2024)|>
  left_join(nat_poll_d_yearly, by = "year")

nopoll_state_pred_2024 <- predict(d_nopoll_state_reg, d_nopoll_state_model_2024, interval = "prediction")

d_nopoll_state_model_2024 <- d_nopoll_state_model_2024 |>
  mutate(predicted_values = nopoll_state_pred_2024[, "fit"],
         lower_bounds = nopoll_state_pred_2024[, "lwr"],
         upper_bounds = nopoll_state_pred_2024[, "upr"])
states_map <- map_data("state")
d_nopoll_state_model_2024$region <- tolower(d_nopoll_state_model_2024$state)
```

## Introduction
It has now been about two weeks since the 2024 election, and, although some states like California and Alaska are still tabulating the final votes, we now have an all-but-final sense of where the national and state-level popular votes have landed. That also means that we can so how accurate, or rather how inaccurate, my predictive models were. Thus, I will begin by recapping my model and my predictions for the national popular vote, state-level results, and the electoral college, before then assessing the accuracy of my model, proposing hypotheses for my model's inaccuracies, and then suggesting ways that I could have built my model better.


## Recap of My Models and Predictions
For my predictions, I created three OLS models: one for the two-party national popular vote, one for the two-party state-level popular vote for states with public polling, and one for the two-party state-level popular vote for states *without* public polling.

### National Popular Vote
My national popular vote model included variables for the October polling average, the September polling average, the (election year) Q2 national GDP growth, and a variable for incumbency. Below was what my model predicted for Harris' two-party national vote share in the 2024 election:

```{r nat pred graphic 1}
knitr::kable(nat_pred_2024, 
             col.names = c("Harris Vote Share", "Lower Bound", "Upper Bound"),
             align = c("c", "c", "c")) 
```

As can be seen, my model predicted incorrectly a Harris national popular vote victory, with the lower bound of the 95% confidence interval being a Trump victory with 53.45% and the upper bound being a Harris landslide with a 57.1% share of the vote.

### State-Level Vote and Electoral College
In terms of my state-level predictions, for the 23 states and two relevant congressional districts for which we had publicly available polling, I used an OLS model with variables for October and September polling averages, one- and two-cycle vote share lag, incumbent status, and the October national polling average. This model yielded the following predictions for these 25 states/districts:

```{r state pred graphic}
knitr::kable(d_state_model_2024 |> 
               select(state, predicted_values, lower_bounds, upper_bounds), 
             col.names = c("State", "Harris Vote Share", "Lower Bound", "Upper Bound"),
             align = c("l", "c", "c", "c")) |>
  row_spec(which(d_state_model_2024$predicted_values >= 50 & d_state_model_2024$predicted_values < 52.5), background = "#ccdfff") |>
  row_spec(which(d_state_model_2024$predicted_values >= 52.5 & d_state_model_2024$predicted_values < 55), background = "#a1c4ff") |>
  row_spec(which(d_state_model_2024$predicted_values >= 55 & d_state_model_2024$predicted_values < 57.5), background = "#6ea3ff") |>
  row_spec(which(d_state_model_2024$predicted_values >= 57.5), background = "#3972d4") |>
  row_spec(which(d_state_model_2024$predicted_values < 50 & d_state_model_2024$predicted_values > 47.5), background = "#ffc7ce") |>
  row_spec(which(d_state_model_2024$predicted_values < 47.5 & d_state_model_2024$predicted_values > 45), background = "#ff9ea9") |>
  row_spec(which(d_state_model_2024$predicted_values <= 45 & d_state_model_2024$predicted_values > 42.5), background = "#ff6e7e") |>
  row_spec(which(d_state_model_2024$predicted_values <= 42.5), background = "#c93c4c")

```

As can be seen, this model predicted a Harris victory in all seven key battleground states, with single-digit margin loses for her in Florida, Texas, Ohio, and Maine's 2nd Congressional district. The upper bounds of this model had Harris also winning these four, while the lower bounds had Trump sweeping all seven key swing states in addition to Minnesota, Nebraska's 2nd Congressional district, New Hampshire, New mexico, and Virginia.

For the remaining states for which there was no public polling, I used a similar OLS model that removed the state-level polling averages but added back in the September national polling average that the national OLS model used. Combining my state-level predictions from these two models

```{r combined state pred map winner}
pred_winner_states_1 <- d_nopoll_state_model_2024 |>
  left_join(states_map, by = "region") |> 
  left_join(d_state_model_2024, by = "state") |>
  mutate(final_prediction = if_else(is.na(predicted_values.y) == 0, predicted_values.y, predicted_values.x),
         Margin = (final_prediction - 50) * 2,
         Win = if_else(Margin > 0, "Harris", "Trump")) |> 
  ggplot(aes(long, lat, group = group)) +
  geom_polygon(aes(fill = Win), color = "black") +  
  scale_fill_manual(values = c("Harris" = "dodgerblue4", "Trump" = "firebrick1")) +  
  theme_void() +
  ggtitle("Predicted Winner by State, 2024") +
  labs(fill = "Winner") +
  theme(aspect.ratio = .60,
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12))

pred_margin_states_1 <- d_nopoll_state_model_2024 |>
  left_join(states_map, by = "region") |> 
  left_join(d_state_model_2024, by = "state") |>
  mutate(final_prediction = if_else(is.na(predicted_values.y) == 0, predicted_values.y, predicted_values.x),
         Margin = (final_prediction - 50) * 2) |>
  ggplot(aes(long, lat, group = group)) +
  geom_polygon(aes(fill = Margin), color = "black") +
  scale_fill_gradientn(colors = c("#9e0213", "#f04658", "#ff9ea9", "#ffc7ce", "white", "#ccdfff", "#a1c4ff", "#4b50e3", "#02079e"), 
                       values = scales::rescale(c(-50, -15, -10, -5, 0, 5, 10, 15, 50)), # Rescale to the range of your data
                       breaks = c(-50, -25, 0, 25, 50),
                       limits = c(-50, 50)) +
  theme_void() +
  ggtitle("Predicted Vote Share by State, 2024") +
  labs(fill = "Margin") +
  theme(aspect.ratio = .60,
        plot.title = element_text(hjust = 0.5, size = 12))

pred_winner_states_1 + pred_margin_states_1
```

This would have yielded a 319-219 electoral college victory for Harris. Clearly, however, this did not quite happen.

```{r combining state-level data, include=FALSE}
d_final_preds <- d_nopoll_state_model_2024 |>
  left_join(d_state_model_2024, by = "state") |>
  mutate(final_prediction = if_else(is.na(predicted_values.y) == 0, predicted_values.y, predicted_values.x),
         final_prediction_margin = (final_prediction - 50) * 2,
         final_lower_bounds = if_else(is.na(lower_bounds.y) == 0, lower_bounds.y, lower_bounds.x),
         final_upper_bounds = if_else(is.na(upper_bounds.y) == 0, upper_bounds.y, upper_bounds.x),
         state = if_else(state == "District Of Columbia", "District of Columbia", state)) |>
  left_join(d_state_2024, by = "state") |>
  select(state, final_prediction, final_prediction_margin, final_lower_bounds, final_upper_bounds, harris_2pv, winner, region) |>
  mutate(harris_2pv = harris_2pv * 100,
         harris_margin = (harris_2pv - 50) * 2,
         pred_error = harris_2pv - final_prediction,
         margin_error = harris_margin - final_prediction_margin)
```


## Assessing My Accuracy
### National Popular Vote
As of today, the New York Times has Harris at 73,846,289 votes nationwide and Trump at 76,488,195, meaning that Harris is (currently) taking about 49.12% of the two-party national popular vote. When looking at how this compares to my national model...

```{r nat pred error graphic}
nat_vote_combined <- data.frame(
  fit = 51.83,
  real_2pv = 49.12,
  lwr = 46.55,
  upr = 57.10
)

knitr::kable(nat_vote_combined, 
             col.names = c("Pred. Harris %", "Real Harris %", "Lower Bound", "Upper Bound"),
             align = c("c", "c", "c", "c")) 
```

... we can see that, although my model did predict a **51.83%** Harris victory that overestimate her share by **2.71%**, this margin *does* squarely fall within my 95% confidence interval, being 2.57% above my lower bound for her vote share.

### State-Level Vote and Electoral College
In terms of my state-level predictions, let's first look at my OLS model for states/districts *with* public polling. Below is the table for these 23 states and two congressional districts, shaded by how much Harris in reality underperformed my model (as seen quantified in the "Error" column).

```{r state pred error graphic}
comb_key_states <- d_state_model_2024 |>
  left_join(d_state_2024, by = "state") |>
  select(state, predicted_values, harris_2pv, lower_bounds, upper_bounds) |>
  mutate(harris_2pv = if_else(state == "Maine Cd 2", 0.45379, harris_2pv),
         harris_2pv = if_else(state == "Nebraska Cd 2", 0.52129, harris_2pv),
         harris_2pv = harris_2pv * 100,
         pred_error = harris_2pv - predicted_values)

knitr::kable(comb_key_states |> 
               select(state, predicted_values, harris_2pv, pred_error, lower_bounds, upper_bounds), 
             col.names = c("State", "Pred. Harris %", "Real Harris %", "Error", "Lower Bound", "Upper Bound"),
             align = c("l", "c", "c", "c", "c")) |>
 row_spec(which(comb_key_states$pred_error < 0 & comb_key_states$pred_error > -1), background = "#ffc7ce") |>
  row_spec(which(comb_key_states$pred_error < -1 & comb_key_states$pred_error > -2), background = "#ff9ea9") |>
  row_spec(which(comb_key_states$pred_error <= -2 & comb_key_states$pred_error > -3), background = "#ff6e7e") |>
  row_spec(which(comb_key_states$pred_error <= -3 & comb_key_states$pred_error > -4), background = "#c93c4c") |>
  row_spec(which(comb_key_states$pred_error <= -4), background = "firebrick")

```

In all of the above states/districts, Harris did worse than my model predicted, with larger, more diverse states such as Florida, California, Texas, and New York having the biggest misses. This said, even my most inaccurate predictions still fell within the 95% confidence interval, although some, California, Florida, and Maryland in particular, were very close, coming within a point of falling below the lower bound.

My second state-level model fared slightly better. Below is a map of how my combined predictions fared against reality, with blue-shaded states representing those which Harris did *better* than what my model(s) predicted and red-shared states being those which she did *worse* than my predictions.

```{r state margin error map}
d_final_preds |>
  left_join(states_map, by = "region") |> 
  left_join(d_state_model_2024, by = "state") |>
  ggplot(aes(long, lat, group = group)) +
  geom_polygon(aes(fill = margin_error), color = "black") +
  scale_fill_gradientn(colors = c("#9e0213", "white", "#02079e"), 
                       limits = c(-10, 10)
                       ) +
  theme_void() +
  ggtitle("Difference Between Predicted and Actual State-Level 2024 Election Margins") +
  labs(fill = "Margin Error") +
  theme(aspect.ratio = .60)
```

In terms of how this translates to who actually won each state, below are the maps of the predicted and actual state-level victors (with Alaska, Hawaii, DC, and the congressional districts in Nebraska and Maine not shown, but not changing between my model compared to the results):

```{r two state winner maps, include=FALSE}
pred_state_winner_map <- d_nopoll_state_model_2024 |>
  left_join(states_map, by = "region") |> 
  left_join(d_state_model_2024, by = "state") |>
  mutate(final_prediction = if_else(is.na(predicted_values.y) == 0, predicted_values.y, predicted_values.x),
         Margin = (final_prediction - 50) * 2,
         Win = if_else(Margin > 0, "Harris", "Trump")) |> 
  ggplot(aes(long, lat, group = group)) +
  geom_polygon(aes(fill = Win), color = "black") +  
  scale_fill_manual(values = c("Harris" = "dodgerblue4", "Trump" = "firebrick1")) +  
  theme_void() +
  ggtitle("Predicted Winner") +
  labs(fill = "Winner") +
  theme(aspect.ratio = .60,
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size = 12))

real_state_winner_map <- d_final_preds |>
  left_join(states_map, by = "region") |> 
  ggplot(aes(long, lat, group = group)) +
  geom_polygon(aes(fill = winner), color = "black") +  
  scale_fill_manual(values = c("DEM" = "dodgerblue4", "REP" = "firebrick1")) +  
  theme_void() +
  ggtitle("Actual Winner") +
  labs(fill = "Winner") +
  theme(aspect.ratio = .60,
        plot.title = element_text(hjust = 0.5, size = 12))
```

```{r}
pred_state_winner_map + real_state_winner_map
```

In terms of quantifying how much my predictions were off, below are the bias, root mean squared error, and mean average error of my combined predictions:

```{r}
d_final_preds |>
  summarize(
    Bias = round(mean(harris_2pv - final_prediction),2),  
    RMSE = round(sqrt(mean((harris_2pv - final_prediction)^2)),2),
    MAE = round(mean(abs(harris_2pv - final_prediction)),2)
  )
```

Although less than ideal, these values are not horribly far off the final results, especially given that this race ended up being less close than 2020 (although a 1-2% error still would have substantially swung the election). This said, there were differences in how accurate my two state-level models were.

In terms of my model for states *with* polling, this model fared notably worse, with higher bias, RMSE, and MAE values compared to my overall state-level predictions:

```{r}
comb_key_states |>
  summarize(
    bias = round(mean(harris_2pv - predicted_values),2),  
    RMSE = round(sqrt(mean((harris_2pv - predicted_values)^2)),2),
    MAE = round(mean(abs(harris_2pv - predicted_values)),2)
  )
```

Meanwhile, the subset of states for which I used the secondary model had almost no bias, and had notably lower RMSE and MAE values:

```{r}
d_nopoll_state_model_2024 |>
  mutate(state = if_else(state == "District Of Columbia", "District of Columbia", state)) |>
  left_join(d_state_2024, by = "state") |>
  select(state, predicted_values, lower_bounds, upper_bounds, harris_2pv, winner, region) |>
  mutate(harris_2pv = harris_2pv * 100,
         harris_margin = (harris_2pv - 50) * 2,
         pred_error = harris_2pv - predicted_values) |>
  summarize(
    bias = round(mean(harris_2pv - predicted_values),2),  
    RMSE = round(sqrt(mean((harris_2pv - predicted_values)^2)),2),
    MAE = round(mean(abs(harris_2pv - predicted_values)),2)
  )
```

I will explore potential reasons for this discrepancy below, but I preliminarily believe that the main reason behind this is that some of the largest shocking electoral shifts of the night happened in large, diverse states like California, New York, Texas, and Florida, all of which are important enough (either electorally or otherwise) for there to be public polling available for, whereas smaller, less diverse, electorally-safe states such as those in the Great Plains and Upper New England did not have polling.


## Potential Explanations for Inaccuracies
### Hypothesis 1: Incumbency
To begin with, I likely did not adequately consider how closely Harris would be tied to Biden in the eyes of the electorate. With the vast majority of Americans believing the country is headed in the "wrong direction" and with President Biden's approval rating being stubbornly abysmal for so long, I should have better taken into account how poorly the electorate may view the current administration, particularly as Harris repeatedly either failed or refused to distance herself from her former running mate. In fact, at one point a few weeks ago, I strongly considered adding into my model a variable for the incumbent president's (election year) June approval rating, but I decided against it because of prior beliefs that President Biden's low approval rating was driven by concerns about age that would not play into a 2024 election with Harris as the Democratic nominee and by Democrats who would vote blue come November regardless. Clearly, this was not the case, and I should not have so quickly dismissed this variable and the notable rightwards shift its inclusion would have caused in my model. This is just one way I could have adjusted my incumbency variable though, and I am sure there could have been ways to make this more nuanced, especially given how complex the "incumbency" question was this year.

### Hypothesis 2: Economics
Americans clearly are still feeling (or at least think they are feeling) the effects of inflation, and exit polls show that the economy was a top issue and that voters who cited the economy as their top issue swung towards Trump. This indicates that GDP was not an accurate indicator of how Americans were viewing the state of the economy. While this may have been a useful factor, it clearly did not capture the whole picture of how Americans have been viewing the economy. For example, I have seen numerous reports that exit polls show voters in swing states saying decisively that their local economy is doing better than four years ago, but that they perceive the *national* economy as doing worse, leading some to deem the current economic outlook in the country a "vibecession," driven by perceptions and not necessarily reality. Because of these above reasons, I could have added in variables for inflation, RDI growth, or consumer sentiment. At one point I toyed with adding in a variable for RDI growth, but I ended up not using it because historically GDP growth had been a more accurate indicator. However, I did not let that stop me from including less historically-accurate variables such as September polling averages in my model since historical accuracy does not necessarily mean that such variables do not add nuance to my model in important ways.

### Hypothesis 3: Demographics
While much more relevant to my state-level model, demographic-related electoral trends clearly played a crucial role in this election with minority voters, especially Hispanic, Asian, and Native American voters, swinging strongly towards the GOP. This has been a common explanation in the media for why states like New York, New Jersey, Illinois, California, Texas, and Florida all swung hard to the right. These demographic trends were somewhat caught preemptively by polling crosstabs, and to correct for this I could have added variables into my state-level models adjusting my polling models by trends with different demographic groups.


## How I Might Have Changed My Model
While I have proposed a number of potential changes to my model above, I would have prioritized adding in variables for RDI growth and consumer sentiment to adjust how my model defined economic perceptions, I would have prioritized adding in some demographic-related weighting mechanism for my state-level models, and I would have added back in that aforementioned June approval rating variable that I almost included in prior blog posts. While my model still likely would have missed in some ways, with the benefit of hindsight I can see why models overestimated Harris and voters' perceptions of the economy, and why my state-level model failed to adequately predict differing trends in some states and regions compared to others.

Altogether, I am fairly proud of how my model held up. While I did incorrectly predict the final result, with one or two minor adjustments the narrow victory for Harris that I predicted in the swing states easily could have shifted to being a narrow victory for Trump instead. I am also very pleased with how surprisingly well my supplementary state-level model held up, albeit with the aforementioned caveat, and I do feel that through these models and predictions I have learned more about America's electorate and what truly seems to matter in elections here.



```{r, include=FALSE}
## build_site(build_rmd=TRUE)
```
