---
title: 'Blog Post 9: Final 2024 Election Prediction'
author: John Kulow
date: '2024-11-04'
slug: week-9-final-2024-election-prediction
categories: []
tags: []
---

## Introduction
For this week's blog post, the day before the 2024 election, I will make my final predictions for the national two-party popular vote, state-level two-party popular vote shares, and the associated electoral college vote. Altogether, my predictions are based off of three models, all multivariate ordinary least squares (OLS) regression models, that hopefully together will paint an accurate picture of what the results will be tomorrow (or whenever we actually end up getting results).  
  
  
  
## National Model and Prediction
### Equation and Variables
Let's begin with my national popular vote prediction. This model is built to predict what percentage of the two-party national popular vote will go to the candidate from the incumbent party. This vote share is the dependent variable. In terms of independent variables for this OLS model, I used the following four:

- ***October Polling Average***: The average of national polls taken in October, as compiled by 538, and weighted by how many weeks were left until the election.
- ***September Polling Average***: The average of national polls taken in *September*, as compiled by 538, and weighted by how many weeks were left until the election.
- ***Quarter 2 GDP Growth***: The percent national GDP growth in Q2 of the respective election year, as provided by the Federal Reserve.
- ***Incumbency***: If the candidate is the incumbent president or not (in addition to being from the incumbent party).

Thus together, the equation for my model is:

$$
\mathbf{National\ Two-Party\ Vote\ Share\ for\ the\ Inc.\ Party} = \mathbf{\beta_0} + \\ 
\mathbf{\beta_1} \textbf{ Oct.\ National\ Polling\ Average} + \mathbf{\beta_2} \textbf{ Sept.\ National\ Polling\ Average} + \\ 
\mathbf{\beta_3} \textbf{ Q2\ GDP\ Growth} + \mathbf{\beta_4} \textbf{ Incumbency} + \mathbf{\epsilon}
$$
  
  
  
### Justification of National Model
**October Polling Average**:
Although the accuracy of political polling has been repeatedly called into question, especially over the past few years following notable mishaps in the polling industry in 2016 and 2020, they remain perhaps our best indicator of what electorates are believing at any given point. This said, polling tends to become more accurate as election day approaches, as found by [Gelman and King
(1993)](https://gking.harvard.edu/files/bj215.pdf). This is why I have weighted my October polling average by each day, favoring polls on days closer to election day.

**September Polling Average**:
I made the decision to include a *September* polling average, despite Gelman and King's findings, because of recent criticisms within the polling industry about two things. First, there are concerns about poll "herding" as the election gets near, with polling agencies being accused of ignoring results that would be outliers and favoring results that either align with other polls or show a closer race. Particularly after the polls supposedly missed in 2016 and 2020, some believe that polling firms are showing a tightening race between Harris and Trump in the last month because these polling agencies got burnt in the last two presidential cycles by overestimating Democrats' strength and would rather underestimate Harris than once again overestimate Democrats. The second reason is that in recent cycles, *particularly* in 2022, we have influxes of partisan-funded polls right before the election with the accused intention of wanting to artificially change poll averages to favor their candidate. Thus, for both of these reasons, I wanted to include a September polling average independent variable, knowing that historically it was a worse predictor, because I am concerned for the above two reasons that October polling may be less accurate this time around. By including an average from September, from before polling began to herd and be biased by partisan polls supposedly, I hope to mitigate the impact on my model that any incorrect last-minute shift in the polls may bring.

**Quarter 2 GDP Growth**:
As always, the economy is front and center in politics in 2024. Voters care about the strength in the economy, and, as [Achen and Bartels
(2014)](https://press.princeton.edu/books/hardcover/9780691169446/democracy-for-realists?srsltid=AfmBOorl6iN4qgweThT5hVNygA3GYiEx8IZYUksJEgexdWLE3Es2imza) note, if the economy is weak then voters tend to blame the incumbent party. If the economy is strong, on the other hand, the incumbent party does relatively better. As explored in previous blog posts, I have settled on using Quarter 2 GDP Growth as a barometer for the economic fundamentals of a campaign both because it has a better predictive track record based off of past elections but also because I believe it is a better, more holistic measurement of strength in the economy rather than other proposed measures such as RDI growth. Quarter 2 is used here because it is recent enough to be in voter's memories, but long enough ago that it defines their perception of the economy under the incumbent party in a way that Q3 would not, as any sudden upswing or downturn in Q3 could likely be waved off by the incumbent party as having been out of their control, or the effects of such a change in the economy may not have had time to trickle down to voters just yet.

**Incumbency**:
Although literature and expert opinions on the role of incumbency are split on whether it has an affect and, if so, whether such an effect is positive or negative. Some claim that the President's superior ability to fund-raise, to gain free media attention, and to otherwise wield the powers of the Presidency to the advantage of them or of their constituents/potential supporters gives the incumbent president an advantage going into reelection. Others claim that voters can blame the president for any problems facing the country. Others believe any such impact one way or the other is minute, irrelevant, or self-balanced. I believe that the role of incumbency does matter, which is why I have included it in my model, and that we need to look no further than President Biden's dramatic exit from the 2024 race this past summer to see the importance of whether an incumbent president, for all their faults or strengths, runs for reelection.
  
  
  
### Regression Table
I based my model off of data spanning from 1968, when polling data from 538 begins, to the 2016 election. I made the decision to exclude the 2020 election because the economic data from Quarter 2 is such an outlier due to the Covid pandemic that it substantively, and in my opinion unrepresentatively, changed my model when included. Thus, based off of the aforementioned four independent variables and the 13 elections spanning 1968-2016, below is the regression table for my model:

```{r setup, warning = FALSE, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, messages = FALSE, warning = FALSE)
```

```{r libraries, include=FALSE}
library(blogdown)
library(car)
library(caret)
library(curl)
library(CVXR)
library(foreign)
library(geofacet)
library(ggpubr)
library(ggthemes)
library(glmnet)
library(gridExtra)
library(haven)
library(janitor)
library(kableExtra)
library(knitr)
library(maps)
library(mgcv)
library(mgcViz)
library(mlr3)
library(patchwork)
library(randomForest)
library(ranger)
library(RColorBrewer)
library(readxl)
library(rstan)
library(scales)
library(sf)
library(shinystan)
library(sjPlot)
library(spData)
library(stargazer)
library(tidygeocoder)
library(tidyverse)
library(tigris)
library(tmap)
library(tmaptools)
library(viridis)
```

```{r importing data, include=FALSE}
####----------------------------------------------------------#
#### Read, merge, and process data.
####----------------------------------------------------------#

# Read popular vote datasets. 
d_popvote <- read_csv("data/popvote_1948_2020.csv")
d_state_popvote <- read_csv("data/state_popvote_1948_2020.csv")
d_state_popvote[d_state_popvote$state == "District of Columbia",]$state <- "District Of Columbia"

# Read elector distribution dataset. 
d_ec <- read_csv("data/corrected_ec_1948_2024.csv")

# Read polling data. 
d_polls <- read_csv("data/national_polls_1968-2024.csv")
d_state_polls <- read_csv("data/state_polls_1968-2024.csv")

# Read turnout data. 
d_turnout <- read_csv("data/state_turnout_1980_2022.csv")

# Read county turnout. 
d_county_turnout <- read_csv("data/county_turnout.csv")

# Read state-level demographics.
d_state_demog <- read_csv("data/demographics.csv")

# Read county demographics. 
d_county_demog <- read_csv("data/county_demographics.csv")

# Read campaign events datasets. 
d_campaign_events <- read_csv("data/campaigns_2016_2024.csv")[,-1]

d_pollav_nat <- read_csv("data/national_polls_1968-2024.csv")
d_pollav_state <- read_csv("data/state_polls_1968-2024.csv")
```

```{r nat model, include=FALSE}
d_poll_nat <- d_pollav_nat |>
  select(year, state, weeks_left, poll_date, poll_support, party) |>
  filter(month(poll_date) %in% c(9, 10)) |>
  group_by(year, party, month = month(poll_date)) |>
  summarize(poll_support = round(weighted.mean(poll_support, weeks_left, na.rm = TRUE), 2), .groups = "drop") |>
  pivot_wider(names_from = month, 
              values_from = poll_support, 
              names_prefix = "month_") |>
  drop_na() |>
  rename(sept_poll = month_9, 
         oct_poll = month_10)

d_fred <- read_csv("data/fred_econ.csv")

d_fred_2 <- d_fred |>
  filter(quarter == 2)

d_nat_model <- d_popvote |> 
  mutate(party = if_else(party == "democrat", "DEM", "REP")) |>
  left_join(d_fred_2, by = "year") |> 
  filter(year > 1967, 
         year != 2020,
         incumbent_party) |>
  mutate(incumbent = as.numeric(incumbent)) |>
  left_join(d_poll_nat, by = c("year", "party"))

d_nat_reg <- lm(pv2p ~ oct_poll + sept_poll + GDP_growth_quarterly + incumbent, 
                data = d_nat_model)
```

```{r nat model table}
tab_model(d_nat_reg, show.se = TRUE, 
          title = "National Model Regression Table", 
          dv.labels = "National 2-Party Vote Share for Incumbent Party")
```

In terms of how we interpret the above coefficients for each of the four IV's:

**October Polling Average**: For every 1-point increase in the incumbent candidate's weighted October national polling average, we can expect an 0.61-point increase in their eventual national two-party vote share. With a p-value of 0.048, this is one of the two statistically significant IV's in my model at a 95% confidence interval.

**September Polling Average**: For every 1-point increase in the incumbent candidate's weighted September national polling average, we can expect an 0.21-point decrease in their eventual national two-party vote share *when controlling for other variables including the October polling average*. This last point is important to note. With a p-value of 0.484, this is not statistically significant, but that does not mean that the inclusion of this variable does not add value to my model, as explained previously.

**Quarter 2 GDP Growth**: For every 1% increase in Quarter 2 GDP growth, there is an associated 0.45-point increase in the incumbent party candidate's November national two-party vote share. With a p-value of 0.025, this is the second of the two statistically significant IV's in my model at a 95% confidence interval.

**Incumbency**: For every 1% increase in "incumbency" there is an associated 1.00-point increase in the incumbent party candidate's national two-party vote share. This may not initially make much sense, but consider that, of course, there is no such thing as a "1% increase in incumbency" as this is a binary variable. With a p-value of 0.456, this too is not statistically significant at a 95% confidence interval, but just like the September polling average, that does not mean it does not add value to my model.
  
  
  
### Validation
For in-sample error, my model's R-squared value of 0.89 means that 89% of changes in national two-party popular vote share is explained by my model, representing a pretty strong correlation. This correlation holds strong when adjusted, with an adjusted R-squared value of 0.84.

To understand my out-of-sample error, I performed 1,000 repetitions of random sampling, with each sample using 7 elections, which is slightly over half of the 13 total in the time frame of my data set. The average mean out-of-sample error, as shown by the black line in the graph below, is ~3.81%, a reasonably low number. 

```{r nat model validation, include=FALSE}
set.seed(02138)

out_samp_errors <- sapply(1:1000, function(i) {
  years_out_samp <- sample(d_nat_model$year, 7)
  mod <- lm(pv2p ~ sept_poll + oct_poll + GDP_growth_quarterly + incumbent, 
            data = d_nat_model[!(d_nat_model$year %in% years_out_samp),])
  out_samp_pred <- predict(mod, d_nat_model[d_nat_model$year %in% years_out_samp,])
  if (any(is.na(out_samp_pred)) || any(is.na(d_nat_model$pv2p[d_nat_model$year %in% years_out_samp]))) {
    return(NA)
  }
  out_samp_truth <- d_nat_model$pv2p[d_nat_model$year %in% years_out_samp]
  mean(abs(out_samp_pred - out_samp_truth))
})
out_samp_errors <- na.omit(out_samp_errors)
mean_out_samp_error <- mean(abs(out_samp_errors))

nat_model_valid_plot <- ggplot() +
  geom_histogram(aes(x = out_samp_errors), fill = "lightgray", binwidth = 0.5) + 
  geom_vline(aes(xintercept = mean_out_samp_error), color = "black") +
  labs(title = "Out-of-Sample Cross Validation Error Distribution",
       x = "Mean Absolute Errors",
       y = "") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 14)
  ) +
  scale_x_continuous(limits = c(0, 20))
```

```{r nat model validation graph}
nat_model_valid_plot
```

The above histogram shows the distribution of these mean errors. I have limited the x-axis of this graph to a reasonable range, so there are one or two outlier data points not shown above, though they are incorporated into the calculation of the average mean absolute error. I have restricted the x-axis so that we can better see that this graph is right-skewed. While there certainly are some outlier data points where my model failed to accurately predict the election, those are far outnumbered by instances of my model accurately capturing the state of the race.
  
  
  
### Final National Two-Party Vote Prediction

Given this model, when we apply this result to the 2024 election between Vice President Harris and Former President Trump, we get the following prediction:

```{r nat pred, include=FALSE}
d_nat_model_2024 <- d_nat_model |>
  filter(year == 2024) |>
  select(sept_poll, oct_poll, GDP_growth_quarterly, juneapp) |>
  mutate(incumbent = 0)

nat_pred_2024 <- predict(d_nat_reg, d_nat_model_2024, interval = "prediction", level = 0.95)
```

```{r nat pred graphic 1}
knitr::kable(nat_pred_2024, 
             col.names = c("Harris Vote Share", "Lower Bound", "Upper Bound"),
             align = c("c", "c", "c")) 
```

As can be seen, my model predicts that Vice President Harris will win the national popular vote with **51.83%** of the vote, a **3.66% margin** over Former President Trump and his **48.17%** share. There is still a relatively large range of possibilities considering today's polarized political climate, with the upper bound of Harris' vote share with a 95% confidence interval being 57.1% and the lower bound being 46.55%, meaning Harris could win by up to a 14.2% margin or lose by up to a 6.9% margin. Thus, to get an even better sense of the state of the race, it is important to now turn to the state level.
  
  
  
## State-Level Model and Predictions
### Equation and Variables

For my state-level analysis, I used two models, with both being OLS regression models built to predict what percentage of the state-level two-party popular vote the Democratic nominee should receive. The first is the primary model, and it uses the following six independent variables:

- **October State-Level Polling Average**: The average of state-level polls taken in October, as compiled by 538, and weighted by how many weeks were left until the election.
- **September State-Level Polling Average**: The average of national polls taken in *September*, as compiled by 538, and weighted by how many weeks were left until the election. For the roughly 3 states (none of which considered swing states) and 1 district (Maine's 2nd) for which there were not any polls released in September but did have October polls, I set their September Polling Average to equal their October average so as to allow them to be included in the model.
- **One Cycle Democratic Vote Lag**: The Democratic candidate's share of the two-party popular vote in the state in the presidential election immediately prior.
- **Two Cycle Democratic Vote Lag**: The Democratic candidate's share of the two-party popular vote in the state in the presidential election two cycles (eight years) ago.
- **Incumbent Party Status**: Whether or not Democrats held the Presidency in the term leading up to the election.
- **National October Polling Average**: The average of *national* polls taken in October, as compiled by 538, and weighted by how many weeks were left until the election.

Taken together, the equation for my primary state-level model is:

$$
\mathbf{State\ Two\ Party\ Vote\ Share\ for\ the\ Democratic\ Nominee} = \mathbf{\beta_0} + \\ 
\mathbf{\beta_1} \textbf{ Oct.\ State\ Level\ Polling\ Avg.} + \mathbf{\beta_2} \textbf{ Sept.\ State\ Level\ Polling\ Avg.} + \\ 
\mathbf{\beta_3} \textbf{ One\ Cycle\ Dem.\ Vote\ Lag} + \mathbf{\beta_4} \textbf{ Two\ Cycle\ Dem.\ Vote\ Lag} + \mathbf{\beta_5} \textbf{ Inc.\ Party\ Status } + \\
\mathbf{\beta_6} \textbf{ Oct.\ National\ Polling\ Avg.} + \mathbf{\epsilon}
$$
  
  
  
### Justification of Model

**State-Level Polling**: As you can see, there are a number of similarities between my national and state models. As previously explained, despite its flaws polling is still a strong indicator of results, and state-level polling is  a good indicator historically of how that state will vote. For the same reasons as my national model, I included variables for both October and September averages. 

**Democratic Vote Lag**: I added two new variables representing past election results in that state. While states certainly swing from year to year, most voters do not change their minds, so the last election, especially in today's polarized political climate, tends to be a good indicator of future results. I included the *two cycle* lag variable for a slightly different reason. Including this variable bakes into my model how much the state shifted left or right going into the last election. While many states may only move slightly or may bounce around depending on the year, some other states (ie. Alaska, Texas, Massachusetts) have fairly consistently trended left in recent cycles, while others (ie. Arkansas, Nevada, Florida) have trended rightwards relative to the nation. These trends tend to be because of gradual demographic changes within these states (such as Texas' rapid growth and diversification) or because of gradual realignments in the composition of the two parties' political bases (such as Democrats' gradual growth among high-education suburbanites). As such, I felt it important to add a second variable to include a measurement of these trends.

**Incumbency**: Here I have factored in incumbency slightly differently than in my national model. Since my national model was based around the incumbent party, it was important to test whether the candidate themselves were an incumbent running for reelection or not. Now that my model is based around *Democratic* vote share, I felt it was more important to focus on party incumbency more generally.

**National Polling**: Obviously this is a state-level model, but I wanted to factor in a variable for national polling averages, since in 2024 we find ourselves in an election where the national polling environment indicates a shift rightwards from 2020, but state-level polling largely has remained stable compared to last cycle's results. Thus, I feel like it is important to test the effects of both since, although they overlap slightly, in tandem they may catch trends previously left unnoticed.
  

Unfortunately however, this model only can be applied to the relatively few states for which we have publicly available polling. There are many states that are considered to be safely in Democrats' or Republicans' aisles that no polling firms has decided to invest resources in polling for non-internal purposes. Thus, knowing that these safe states will not affect my electoral college prediction I decided to make a ***second***, slightly simpler model for them. This supplementary OLS model drops the variables for October and September state-level polling averages, but it adds back in the national September polling average IV from my national model so as to replicate the potential September effects I explained above. In my primary state-level model, I decided that the inclusion of only the state-level September polling was sufficient and thus did not include the national September polling average in that primary model.
  
  
  
### Regression Table

For my state-level models, I based them off of elections spanning 2000-2020. I chose to start at 2000 because I felt that this was the beginning of a relatively modern, slightly more calcified era of electoral politics. Furthermore, now that I have dropped Q2 GDP growth data from my state-level prediction, I can include 2020 data again without incorporating severe outlier data points like I would have in my national model. Thus, based off of the six above independent variables and the 290 state-level elections for which we have polling for presidential elections in this time frame, below is the regression table for my primary state-level model.

```{r state model, include=FALSE}
nat_poll_d_yearly <- d_pollav_nat |>
  select(year, state, weeks_left, poll_date, poll_support, party) |>
  filter(month(poll_date) %in% c(9, 10)) |>
  group_by(year, party, month = month(poll_date)) |>
  summarize(poll_support = round(weighted.mean(poll_support, weeks_left, na.rm = TRUE), 2), .groups = "drop") |>
  pivot_wider(names_from = month, 
              values_from = poll_support, 
              names_prefix = "month_") |>
  drop_na() |>
  rename(nat_sept_poll = month_9, 
         nat_oct_poll = month_10) |>
  filter(party == "DEM")

d_poll_state <- d_pollav_state |>
  select(year, state, weeks_left, poll_date, poll_support, party) |>
  filter(month(poll_date) %in% c(9, 10),
         party == "DEM", 
         year > 1999) |>
  group_by(year, state, month = month(poll_date)) |>
  summarize(poll_support = round(weighted.mean(poll_support, weeks_left, na.rm = TRUE), 2), .groups = "drop") |>
  pivot_wider(names_from = month, 
              values_from = poll_support, 
              names_prefix = "month_") |>
  left_join(d_state_popvote, by = c("year", "state")) |>
  select(year, state, month_9, month_10, D_pv2p, D_pv2p_lag1, D_pv2p_lag2) |>
  drop_na() |>
  rename(sept_poll = month_9, 
         oct_poll = month_10) |>
  left_join(d_fred_2, by = "year") |>
  mutate(d_incumbent = case_when(year == "2000" ~ 1,
                                 year == "2004" ~ 0,
                                 year == "2008" ~ 0,
                                 year == "2012" ~ 1,
                                 year == "2016" ~ 1,
                                 year == "2020" ~ 0,
                                 year == "2024" ~ 1),
         D_pv2p_lag_shift = D_pv2p_lag1 - D_pv2p_lag2) |>
  left_join(nat_poll_d_yearly, by = "year")

d_state_reg <- lm(D_pv2p ~ oct_poll + sept_poll + D_pv2p_lag1 + D_pv2p_lag2 + d_incumbent + nat_oct_poll,
                data = d_poll_state)
```

```{r state model reg}
tab_model(d_state_reg, show.se = TRUE, 
          title = "(Primary) State Model Regression Table", 
          dv.labels = "State 2-Party Vote Share for Democrats")
```

**October State-Level Polling Average**: For every 1-point increase in the Democratic candidate's weighted October state-level polling average, we can expect an 0.95-point increase in their eventual state-level two-party vote share. With a p-value of <0.001, this is one of the three statistically significant IV's in my primary state-level model at a 95% confidence interval.

**September State-Level Polling Average**: For every 1-point increase in the Democratic candidate's weighted September state-level polling average, we can expect an 0.18-point decrease in their eventual state-level two-party vote share *when controlling for other variables including the October polling average*. This last point is important to note. With a p-value of 0.118, this is not statistically significant at a 95% confidence interval, but, like the September polling average in my national model that does not mean that the inclusion of this variable does not add value to my model.

**One Cycle Democratic Vote Lag**: For ever 1-point increase in Democratic share of the two-party popular vote in the state in the *last* election, we can expect a 0.38-point increase in the Democrat's eventual state-level two-party vote share for the current year. With a p-value of <0.001, this is one of the three statistically significant IV's in my primary state-level model at a 95% confidence interval.

**Two Cycle Democratic Vote Lag**: For ever 1-point increase in Democratic share of the two-party popular vote in the state in the election *two cycles prior*, we can expect an 0.01-point decrease in the Democrat's eventual state-level two-party vote share for the current year. With a p-value of 0.835, this is not statistically significant at a 95% confidence interval. In fact, this variable has very little effect at all in my model, but I felt that it was important to include it both because I believe my reasoning is sound for doing so and because it plays a more important role in my supplementary model when my model will have to predict the trend of a state relative to the nation without being able to rely on state-level polling data like this primary model does. 

**Democratic Incumbency**: For every 1% increase in "incumbency" there is an associated 0.95-point decrease in the Democratic party candidate's state-level two-party vote share. Like with the incumbency variable in my national model, there is, of course, no such thing as a "1% increase in incumbency" as this is a binary variable. Nonetheless, it is still important to understand that incumbency tends to hurt Democrats in this model. With a p-value of 0.07, this too is not statistically significant at a 95% confidence interval, albeit narrowly so, but just like the September polling average and two cycle democratic vote lag, that does not mean it does not add value to my model.

**October National Polling Average**: For every 1-point increase in the Democratic candidate's weighted October *national* polling average, we can expect an 0.22-point decrease in their eventual state-level two-party vote share. With a p-value of 0.033, this is one of the three statistically significant IV's in my primary state-level model at a 95% confidence interval.

In terms of my supplementary model, below is the associated regression table:

```{r state model 2, include=FALSE}
d_nopoll_states <- d_state_popvote |>
  select(year, state, D_pv2p, D_pv2p_lag1, D_pv2p_lag2) |>
  left_join(d_fred_2, by = "year") |>
  mutate(d_incumbent = case_when(year == "1948" ~ 1,
                                 year == "1952" ~ 1,
                                 year == "1956" ~ 0,
                                 year == "1960" ~ 0,
                                 year == "1964" ~ 1,
                                 year == "1968" ~ 1,
                                 year == "1972" ~ 0,
                                 year == "1976" ~ 0,
                                 year == "1980" ~ 1,
                                 year == "1984" ~ 0,
                                 year == "1988" ~ 0,
                                 year == "1992" ~ 0,
                                 year == "1996" ~ 1,
                                 year == "2000" ~ 1,
                                 year == "2004" ~ 0,
                                 year == "2008" ~ 0,
                                 year == "2012" ~ 1,
                                 year == "2016" ~ 1,
                                 year == "2020" ~ 0,
                                 year == "2024" ~ 1)) |>
  filter(year > 2000) |>
  left_join(nat_poll_d_yearly, by = "year")

d_nopoll_state_reg <- lm(D_pv2p ~ D_pv2p_lag1 + D_pv2p_lag2 + d_incumbent + nat_oct_poll + nat_sept_poll,
                data = d_nopoll_states)
```

```{r state model 2 reg}
tab_model(d_nopoll_state_reg, show.se = TRUE, 
          title = "(Supplementary) State Model Regression Table", 
          dv.labels = "State 2-Party Vote Share for Democrats")
```

I will not go into much detail at all about these numbers since this model is supplementary in nature, but it is important to note that four out of the five independent variables are statistically significant, and with the fifth, the two-cycle Democratic party vote share, being on the border of being so and being notably more impactful on the model's outputs than it was in my primary model, indicating that it is serving its intended purpose of predicting longer-term trends.
  
  
  
### Validation

For in-sample error, my primary state-level model’s R-squared value of 0.93 means that 93% of changes in state-level Democratic two-party popular vote share is explained by my model, representing a pretty strong correlation. This correlation holds strong when adjusted, with an adjusted R-squared value also of 0.93. These numbers are very similar for my supplementary model, with R-squared and adjusted R-squared values of 0.94 and 0.93 respectively.

To understand my out-of-sample error in my primary state-level model, I performed 1,000 repetitions of random sampling, with each sample using 3 election years, which is half of the 6 total in the time frame of my data set. The average mean out-of-sample error, as shown by the black line in the graph below, is ~3.98%, a reasonably low number.

```{r state model validation, include=FALSE}
set.seed(02138)

out_samp_errors_st <- sapply(1:1000, function(i) {
  years_out_samp_st <- sample(d_poll_state$year, 3)
  mod_st <- lm(D_pv2p ~ sept_poll + oct_poll + D_pv2p_lag1 + D_pv2p_lag2 + d_incumbent + nat_oct_poll, 
            data = d_poll_state[!(d_poll_state$year %in% years_out_samp_st),])
  out_samp_pred_st <- predict(mod_st, d_poll_state[d_poll_state$year %in% years_out_samp_st,])
  if (any(is.na(out_samp_pred_st)) || any(is.na(d_poll_state$D_pv2p[d_poll_state$year %in% years_out_samp_st]))) {
    return(NA)
  }
  out_samp_truth_st <- d_poll_state$D_pv2p[d_poll_state$year %in% years_out_samp_st]
  mean(abs(out_samp_pred_st - out_samp_truth_st))
})
out_samp_errors_st <- na.omit(out_samp_errors_st)
mean_out_samp_error_st <- mean(abs(out_samp_errors_st))

state_model_valid_plot <- ggplot() +
  geom_histogram(aes(x = out_samp_errors_st), fill = "lightgray", binwidth = 0.25) + 
  geom_vline(aes(xintercept = mean_out_samp_error_st), color = "black") +
  labs(title = "Out-of-Sample Cross Validation Errors",
       x = "Mean Absolute Errors",
       y = "") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 14)
  ) +
  scale_x_continuous(limits = c(0, 8))
```

```{r state validation graph}
state_model_valid_plot
```

The above histogram shows the distribution of these mean errors, which is slightly right-skewed. While there certainly are some outlier data points that push above 5% where my model failed to accurately predict the election within a reasonable margin of error, most scenarios fall below that threshold, and there are no outlier data points where my model missed by an average of over 7 points. I will not bother repeating this out-of-sample validation for my supplementary model.
  
  
  
### Final State-Level Two-Party Vote Prediction

Given the primary model, when we apply this result to the 2024 election between Vice President Harris and Former President Trump, we get the following predictions for states that have had publically available polling:
```{r state pred, include=FALSE}
d_state_model_2024 <- d_pollav_state |>
  select(year, state, weeks_left, poll_date, poll_support, party) |>
  filter(month(poll_date) %in% c(9, 10),
         party == "DEM") |>
  group_by(year, state, month = month(poll_date)) |>
  summarize(poll_support = round(weighted.mean(poll_support, weeks_left, na.rm = TRUE), 2), .groups = "drop") |>
  pivot_wider(names_from = month, 
              values_from = poll_support, 
              names_prefix = "month_") |>
  left_join(d_state_popvote, by = c("year", "state")) |>
  arrange(state, year) |>
  group_by(state) |>
  mutate(D_pv2p_lag1 = lag(D_pv2p, n = 1),
         D_pv2p_lag2 = lag(D_pv2p, n = 2)) |>
  ungroup() |>
  select(year, state, month_9, month_10, D_pv2p, D_pv2p_lag1, D_pv2p_lag2) |>
  rename(sept_poll = month_9, 
         oct_poll = month_10) |>
  left_join(d_fred_2, by = "year") |>
  mutate(d_incumbent = case_when(year == "2008" ~ 0,
                                 year == "2012" ~ 1,
                                 year == "2016" ~ 1,
                                 year == "2020" ~ 0,
                                 year == "2024" ~ 1)) |>
  filter(year == "2024") |>
  mutate(sept_poll = case_when(is.na(sept_poll) == 1 ~ oct_poll,
                               TRUE ~ sept_poll),
         D_pv2p_lag1 = case_when(state == "Nebraska Cd 2" ~ 53.27,
                                 state == "Maine Cd 2" ~ 46.86,
                                 TRUE ~ D_pv2p_lag1),
         D_pv2p_lag2 = case_when(state == "Nebraska Cd 2" ~ 48.99,
                                 state == "Maine Cd 2" ~ 45.12,
                                 TRUE ~ D_pv2p_lag2)) |>
  left_join(nat_poll_d_yearly, by = "year")

state_pred_2024 <- predict(d_state_reg, d_state_model_2024, interval = "prediction")

d_state_model_2024 <- d_state_model_2024 |>
  mutate(predicted_values = state_pred_2024[, "fit"],
         lower_bounds = state_pred_2024[, "lwr"],
         upper_bounds = state_pred_2024[, "upr"])
```

```{r state pred graphic}
knitr::kable(d_state_model_2024 |> 
               select(state, predicted_values, lower_bounds, upper_bounds), 
             col.names = c("State", "Harris Vote Share", "Lower Bound", "Upper Bound"),
             align = c("l", "c", "c", "c")) |>
  row_spec(which(d_state_model_2024$predicted_values >= 50 & d_state_model_2024$predicted_values < 52.5), background = "#ccdfff") |>
  row_spec(which(d_state_model_2024$predicted_values >= 52.5 & d_state_model_2024$predicted_values < 55), background = "#a1c4ff") |>
  row_spec(which(d_state_model_2024$predicted_values >= 55 & d_state_model_2024$predicted_values < 57.5), background = "#6ea3ff") |>
  row_spec(which(d_state_model_2024$predicted_values >= 57.5), background = "#3972d4") |>
  row_spec(which(d_state_model_2024$predicted_values < 50 & d_state_model_2024$predicted_values > 47.5), background = "#ffc7ce") |>
  row_spec(which(d_state_model_2024$predicted_values < 47.5 & d_state_model_2024$predicted_values > 45), background = "#ff9ea9") |>
  row_spec(which(d_state_model_2024$predicted_values <= 45 & d_state_model_2024$predicted_values > 42.5), background = "#ff6e7e") |>
  row_spec(which(d_state_model_2024$predicted_values <= 42.5), background = "#c93c4c")

```

When you combine these results with my supplementary model's predictions for states without polling data, you get the below map of my prediction for the nation, with states shaded by whether they will have voted for or against Harris:

```{r combined state pred and graphic setup, include=FALSE}
states <- unique(d_state_popvote$state)  # List of all 50 states
year_2024 <- 2024
new_rows <- expand.grid(year = year_2024, state = states)

# Join with historical data to get D_pv2p_lag1 and D_pv2p_lag2
new_rows <- new_rows |>
  left_join(d_state_popvote |>
              filter(year %in% c(2020, 2016)) |>
              select(state, year, D_pv2p) |>
              arrange(state, year) |>
              group_by(state) |>
              summarize(
                D_pv2p_lag1 = D_pv2p[year == 2020],  # Lag 1 from 2020
                D_pv2p_lag2 = D_pv2p[year == 2016]   # Lag 2 from 2016
              ),
            by = "state")

# Add new rows to the original dataset
d_state_popvote_2024 <- bind_rows(d_state_popvote, new_rows)

d_nopoll_state_model_2024 <- d_state_popvote_2024 |>
  select(year, state, D_pv2p, D_pv2p_lag1, D_pv2p_lag2) |>
  left_join(d_fred_2, by = "year") |>
  mutate(d_incumbent = case_when(year == "2016" ~ 1,
                                 year == "2020" ~ 0,
                                 year == "2024" ~ 1),
         d_incumbent_cand = case_when(year == "2016" ~ 0,
                                 year == "2020" ~ -1,
                                 year == "2024" ~ 0)) |>
  filter(year == 2024)|>
  left_join(nat_poll_d_yearly, by = "year")

nopoll_state_pred_2024 <- predict(d_nopoll_state_reg, d_nopoll_state_model_2024, interval = "prediction")

d_nopoll_state_model_2024 <- d_nopoll_state_model_2024 |>
  mutate(predicted_values = nopoll_state_pred_2024[, "fit"],
         lower_bounds = nopoll_state_pred_2024[, "lwr"],
         upper_bounds = nopoll_state_pred_2024[, "upr"])
states_map <- map_data("state")
d_nopoll_state_model_2024$region <- tolower(d_nopoll_state_model_2024$state)
```

```{r list of state model 2 results, include=FALSE}
knitr::kable(d_nopoll_state_model_2024 |> 
               select(state, predicted_values, lower_bounds, upper_bounds), 
             col.names = c("State", "Harris Vote Share", "Lower Bound", "Upper Bound")) |>
  row_spec(which(d_nopoll_state_model_2024$predicted_values >= 50 & d_state_model_2024$predicted_values < 52.5), background = "#ccdfff") |>
  row_spec(which(d_nopoll_state_model_2024$predicted_values >= 52.5 & d_state_model_2024$predicted_values < 55), background = "#a1c4ff") |>
  row_spec(which(d_nopoll_state_model_2024$predicted_values >= 55 & d_state_model_2024$predicted_values < 57.5), background = "#6ea3ff") |>
  row_spec(which(d_nopoll_state_model_2024$predicted_values >= 57.5), background = "#3972d4") |>
  row_spec(which(d_nopoll_state_model_2024$predicted_values < 50 & d_state_model_2024$predicted_values > 47.5), background = "#ffc7ce") |>
  row_spec(which(d_nopoll_state_model_2024$predicted_values < 47.5 & d_state_model_2024$predicted_values > 45), background = "#ff9ea9") |>
  row_spec(which(d_nopoll_state_model_2024$predicted_values <= 45 & d_state_model_2024$predicted_values > 42.5), background = "#ff6e7e") |>
  row_spec(which(d_nopoll_state_model_2024$predicted_values <= 42.5), background = "#c93c4c")

```

```{r combined state pred map winner}
d_nopoll_state_model_2024 |>
  left_join(states_map, by = "region") |> 
  left_join(d_state_model_2024, by = "state") |>
  mutate(final_prediction = if_else(is.na(predicted_values.y) == 0, predicted_values.y, predicted_values.x),
         Margin = (final_prediction - 50) * 2,
         Win = if_else(Margin > 0, "Harris", "Trump")) |> 
  ggplot(aes(long, lat, group = group)) +
  geom_polygon(aes(fill = Win), color = "black") +  
  scale_fill_manual(values = c("Harris" = "dodgerblue4", "Trump" = "firebrick1")) +  
  theme_void() +
  ggtitle("Predicted Presidential Vote Winner by State for the 2024 Election") +
  labs(fill = "Winner") +
  theme(aspect.ratio = .60)
```

Not visualized here are Nebraska's 2nd Congressional District and Maine's 2nd Congressional District, which go to Harris and Trump respectively, nor Trump-won Alaska or Harris-won Hawaii. Altogether, this map predicts a Harris victory in the electoral college 319-219 votes. Once again, however, this is a close election, and the table above with the predicted results for states with polling (which includes all key swing states/districts), shows that either party is easily within the confidence bounds of winning 270 electoral votes. The map below, which shows Harris and Trump's predicted margins in states, helps visualize how close this election is in those key states.

```{r combined state pred map}
d_nopoll_state_model_2024 |>
  left_join(states_map, by = "region") |> 
  left_join(d_state_model_2024, by = "state") |>
  mutate(final_prediction = if_else(is.na(predicted_values.y) == 0, predicted_values.y, predicted_values.x),
         Margin = (final_prediction - 50) * 2) |>
  ggplot(aes(long, lat, group = group)) +
  geom_polygon(aes(fill = Margin), color = "black") +
  scale_fill_gradientn(colors = c("#9e0213", "#f04658", "#ff9ea9", "#ffc7ce", "white", "#ccdfff", "#a1c4ff", "#4b50e3", "#02079e"), 
                       values = scales::rescale(c(-50, -15, -10, -5, 0, 5, 10, 15, 50)), # Rescale to the range of your data
                       breaks = c(-50, -25, 0, 25, 50),
                       limits = c(-50, 50)) +
  theme_void() +
  ggtitle("Predicted Presidential Vote Share by State for the 2024 Election") +
  labs(fill = "Harris Margin") +
  theme(aspect.ratio = .60)
```
  
  
  
## Conclusion

Overall, if my models are correct Harris would narrowly win the crucial swing states and national popular vote, making her the first ever Madam President-Elect, but we will soon see whether we have to wait longer to utter those words.













```{r, include=FALSE}
## build_site(build_rmd=TRUE)
```

